\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{xcolor}

\newcommand{\fill}[1]{\textcolor{red}{\textbf{[#1]}}}

\title{Mechanistic Interpretability of Deception in Language Models\\Trained on Social Deduction Games}
\author{Timothy Obiso \\ AI Safety, Ethics, and Society Final Project}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
I study whether language models fine-tuned on social deduction game transcripts develop internally interpretable representations of deception. I fine-tune Llama~3.1~8B with QLoRA on Werewolf and social reasoning game data, jointly optimizing a causal language modeling objective and a binary deception classification loss. I probe all layers for deception-related signal across five training checkpoints (200--800 steps), finding that linear probes consistently achieve 94--97\% accuracy compared to 50--60\% for a shuffled-label control baseline. The deception feature is nonlinearly encoded at the embedding layer but becomes linearly accessible by layer~5, with a ``write once, read anywhere'' pattern across the residual stream. I then provide causal evidence by applying contrastive activation steering, showing that steering monotonically shifts probe predictions from 0\% to 100\% deception probability.
\end{abstract}

\section{Introduction}

A central concern in AI safety is whether language models can engage in deception---producing outputs that systematically mislead while pursuing an objective \citep{park2023ai, hubinger2024sleeper}. Understanding the internal mechanisms by which models represent and produce deceptive outputs is a prerequisite for building reliable detectors and safeguards.

Social deduction games (Werewolf, Among Us, Mafia) provide a natural laboratory for studying deception. Players assigned hidden roles must deceive or detect deception through dialogue. Game transcripts thus contain ground-truth labels for deceptive intent, tied to known roles, in a constrained but realistic setting.

I ask two questions:
\begin{enumerate}
    \item Do fine-tuned language models develop \emph{linearly accessible} internal representations of deception?
    \item Is it possible to \emph{causally} intervene on these representations to steer model behavior?
\end{enumerate}

\section{Related Work}

\textbf{Mechanistic interpretability.} Linear probing \citep{alain2016understanding, belinkov2017neural} tests whether features are linearly decodable from intermediate representations. Activation steering \citep{turner2023activation, li2024inference} provides causal evidence by adding direction vectors to residual stream activations at inference time.

\textbf{Deception in LLMs.} \citet{park2023ai} survey deceptive capabilities of LLMs. \citet{hubinger2024sleeper} demonstrate that models can be trained to exhibit deceptive alignment. \citet{pacchiardi2023catch} propose benchmarks for detecting lying in LLMs.

\textbf{Social deduction games as testbeds.} \citet{lai2022werewolf} release the Werewolf Among Us dataset with strategy annotations. The SocialMaze benchmark \citep{socialmaze2024} provides multi-round social deduction dialogues with role-based reasoning.

\section{Methods}

\subsection{Data}

I use the Werewolf Among Us dataset \citep{lai2022werewolf}, containing One Night Ultimate Werewolf games with player roles and dialogue annotated with strategic speech acts (Identity Declaration, Accusation, Defense, Evidence).

Each utterance is labeled as deceptive if the speaker holds an evil role (Werewolf, Minion) and uses a strategic speech act. Input text takes the form:

\begin{verbatim}
[werewolf] PlayerName: utterance text
\end{verbatim}

The player's role is not included in the input text---it is used only for label generation, ensuring that probes must detect deception from behavioral features rather than surface-level role tokens.

The combined dataset contains approximately 26{,}500 training examples, with validation and test splits drawn from the same sources. The deception rate varies by game type; I use the combined label distribution as-is without rebalancing.

\subsection{Model and Training}

I fine-tune Llama~3.1~8B-Instruct using QLoRA \citep{dettmers2023qlora} with rank $r{=}16$, $\alpha{=}32$, targeting all attention and MLP projection matrices. I jointly optimize two losses:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{\text{LM}} + \lambda \cdot \mathcal{L}_{\text{deception}}
\end{equation}
where $\mathcal{L}_{\text{LM}}$ is the standard causal language modeling loss and $\mathcal{L}_{\text{deception}}$ is a binary cross-entropy loss computed from mean-pooled last-layer hidden states passed through a classification head (two hidden layers: $4096 \to 1024 \to 256 \to 1$). I set $\lambda = 1.0$.

Training hyperparameters: 1 epoch, batch size 16 (with 2 gradient accumulation steps for an effective batch size of 32), learning rate $2 \times 10^{-4}$, cosine schedule with 3\% warmup, max sequence length 512.

Note that 8B parameters may be undersized for developing rich internal representations of deception; larger models (70B+) likely have greater capacity for separating deceptive from honest reasoning in their residual streams. I use 8B due to compute constraints, and this should be considered when interpreting results.

\subsection{Probing}
\label{sec:probes}

For each transformer layer $\ell \in \{0, \ldots, L\}$, I extract mean-pooled hidden states across the sequence dimension. I train three probes on an 80/20 shuffled split of 1000 test examples:

\begin{itemize}
    \item \textbf{Linear probe}: logistic regression ($h_\ell \to \sigma(w^\top h_\ell + b)$).
    \item \textbf{MLP probe}: two hidden layers ($128 \to 64$), trained for 200 epochs with early stopping.
    \item \textbf{Control probe}: identical linear probe trained on \emph{randomly shuffled} labels, establishing the accuracy achievable from spurious correlations in high-dimensional space.
\end{itemize}

\subsection{Activation Steering}

I use two complementary steering methods at the layer $\ell^*$ identified by probing:

\textbf{Contrastive steering.} I compute a steering vector $v = \bar{h}_{\text{deceptive}} - \bar{h}_{\text{honest}}$ from 20 deceptive and 20 honest examples at layer $\ell^*$, normalized to unit length. At inference, I intervene on the residual stream:
\begin{equation}
    h_\ell' = h_\ell + \alpha \cdot v
\end{equation}
for steering strengths $\alpha \in \{-3, -2, -1, -0.5, 0, 0.5, 1, 2, 3\}$.

\textbf{Probe-direction steering.} I use the trained linear probe's weight vector $w$ (normalized) as the steering direction. This tests whether the direction the probe identified as encoding deception can causally influence model outputs.

For both methods, I measure:
\begin{itemize}
    \item \textbf{Logit divergence}: mean absolute difference in output logits from the unsteered baseline.
    \item \textbf{Probe score} (contrastive only): the deception probe's predicted probability on the steered hidden states---this is not circular because the steering vector and probe are computed independently.
\end{itemize}

\section{Results}

\subsection{Training}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\textwidth]{../analysis_results/training_curves.png}
    \caption{Combined training loss (LM + deception) over the course of training.}
    \label{fig:training}
\end{figure}

Figure~\ref{fig:training} shows the combined loss decreasing over training, confirming that both the LoRA adapter and the deception classification head receive gradient signal and learn jointly.

\subsection{Probe Accuracy by Layer}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{../analysis_results/probe_accuracy_all_checkpoints.png}
    \caption{Probe accuracy across layers at five training checkpoints (200--800 steps). Left: linear probe. Center: MLP probe. Right: control probe (shuffled labels). The gray dotted line indicates chance (50\%).}
    \label{fig:probes}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{../analysis_results/peak_accuracy_over_training.png}
    \caption{Peak probe accuracy over training. Linear and MLP probes both reach 95--97\% by step~300, while the control baseline stays near chance.}
    \label{fig:peak}
\end{figure}

Figure~\ref{fig:probes} shows probe accuracy across all 33 layers at five training checkpoints (200, 300, 400, 500, and 800 steps), and Figure~\ref{fig:peak} summarizes the peak accuracy trend. Table~\ref{tab:probes} reports the top-5 layers at each checkpoint. Across all checkpoints, the linear probe reaches 94--97\% while the shuffled-label control baseline averages 50--60\%, yielding a 34--47 percentage point gap that confirms the deception signal is genuine and not an artifact of high-dimensional spurious correlations.

\begin{table}[h]
\centering
\small
\begin{tabular}{cl ccc}
\toprule
Step & Layer & Linear & MLP & Control \\
\midrule
\multirow{3}{*}{200} & 24 & 94.0 & 91.0 & 61.0 \\
 & 25 & 94.0 & 90.5 & 60.0 \\
 & 31 & 94.0 & 92.0 & 59.5 \\
\midrule
\multirow{3}{*}{300} & 5 & 97.0 & 95.0 & 56.5 \\
 & 7 & 97.0 & 96.0 & 56.5 \\
 & 9 & 97.0 & 96.0 & 57.0 \\
\midrule
\multirow{3}{*}{400} & 5 & 95.5 & 95.0 & 50.5 \\
 & 8 & 95.5 & 95.0 & 50.0 \\
 & 9 & 95.5 & 95.5 & 52.0 \\
\midrule
\multirow{3}{*}{500} & 5 & 94.0 & 93.5 & 61.5 \\
 & 10 & 94.0 & 93.5 & 59.0 \\
 & 20 & 94.0 & 93.5 & 60.5 \\
\midrule
\multirow{3}{*}{800} & 32 & 95.0 & 94.5 & 51.5 \\
 & 5 & 94.5 & 93.5 & 55.5 \\
 & 17 & 94.5 & 95.0 & 55.5 \\
\bottomrule
\end{tabular}
\caption{Top-3 layers by linear probe accuracy (\%) at each training checkpoint.}
\label{tab:probes}
\end{table}

Four findings stand out.

First, at every checkpoint the MLP probe achieves $>$93\% accuracy from layer~0 onward, while the linear probe at layer~0 ranges from 54--60\%. This gap indicates that deception-related information is present in the embedding layer but encoded in a \emph{nonlinear} geometry that only the MLP probe can recover. As activations flow through successive transformer layers, the linear probe rapidly closes the gap, reaching parity by layer~3--5. This pattern---nonlinear in early layers, linearly separable in later layers---suggests the network progressively reorganizes the deception feature into a linearly accessible direction, consistent with prior findings that features become more linearly decodable in deeper layers \citep{alain2016understanding}. This is also evidence that the model genuinely processes deception-related information rather than memorizing surface patterns, which would be linearly decodable from the embedding layer onward.

Second, once the linear probe reaches peak accuracy (around layer~5), it remains at a near-constant plateau through the remaining layers. This ``write once, read anywhere'' pattern is consistent with the residual stream acting as persistent memory: the deception feature is written into the residual stream by early layers and preserved through the rest of the network.

Third, peak linear probe accuracy follows a non-monotonic trajectory across training: 94.0\% at 200 steps, rising to 97.0\% at 300 steps, then settling to 94--95.5\% at later checkpoints. The peak at 300 steps may reflect a transient phase where the deception feature is maximally separated before the model redistributes capacity to other objectives (language modeling). The control baseline is lowest at 400 steps ($\sim$50\%, i.e., chance), suggesting that mid-training produces the cleanest separation between genuine and spurious signal.

Fourth, the MLP probe slightly underperforms the linear probe at peak layers in most checkpoints, likely due to overfitting on the smaller probe training set (800 examples). The key conclusion is that deception is \emph{linearly accessible}: a single hyperplane in activation space suffices.

\subsection{Steering Results}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{../analysis_results_400/steering_sweep.png}
    \caption{Left: contrastive steering strength vs.\ logit divergence (blue) and probe score (red). Right: probe-direction steering strength vs.\ logit divergence.}
    \label{fig:steering}
\end{figure}

Figure~\ref{fig:steering} shows the steering results at the 400-step checkpoint (layer~4). For contrastive steering, logit divergence is approximately constant ($\sim$2.3) for all nonzero $\alpha$, indicating that even small steering perturbations substantially alter the output distribution. The probe score varies monotonically across the full range of $\alpha$, sweeping from $\sim$1.0 at one extreme to $\sim$0 at the other (baseline 0.60 at $\alpha = 0$). This monotonic trend is the central causal finding: the contrastive direction (computed from mean activation differences) systematically shifts the probe's reading (trained via independent logistic regression), confirming that both methods identify the same underlying feature in the residual stream.

For probe-direction steering, logit divergence increases monotonically with $|\alpha|$ (0 to 1.375 at $\alpha = \pm 3$) and is roughly symmetric around $\alpha = 0$. The smaller magnitude compared to contrastive steering (1.4 vs.\ 2.3) suggests that the probe's weight vector captures a component of the deception feature but not its full causal structure.

\section{Discussion}

\subsection{Model Scale}

An important caveat is that 8B parameters may be insufficient for the model to develop the kind of rich, separable deception representations that interpretability tools can cleanly identify. Prior work on deceptive alignment \citep{hubinger2024sleeper} uses models of comparable or larger scale, but the complexity of ``understanding'' deception---as opposed to pattern-matching on game-specific cues---may require greater model capacity. That said, the 97\% probe accuracy at 8B scale demonstrates that even a relatively small model develops a cleanly linearly separable deception feature. Whether this reflects genuine strategic reasoning or sophisticated pattern matching on utterance style remains an open question that larger-scale experiments could help resolve.

\subsection{Limitations}

\begin{itemize}
    \item \textbf{Mean pooling.} I pool hidden states across the full sequence (including padding), which dilutes position-specific signal. Token-level probing at the speaker's utterance could yield stronger results.
    \item \textbf{Domain specificity.} Results may not transfer to real-world deception. Social deduction games have structured, formulaic deception patterns.
    \item \textbf{Label noise.} Not all utterances by evil-role players are deceptive, and the heuristic labeling (role + strategic speech act) is imperfect.
    \item \textbf{Single model.} I study one model size (8B) with one fine-tuning method (QLoRA). Scaling behavior is unknown.
\end{itemize}

\subsection{Implications for AI Safety}

The existence of linearly decodable deception features at 97\% accuracy suggests that runtime monitoring via lightweight probes is feasible---a single linear classifier on mid-layer activations suffices to flag deceptive outputs with high reliability. The causal steering results further suggest that these features could be suppressed as a safety intervention, though the practical implications of steering on open-ended generation require further study.

\subsection{Ethical Considerations}

This work studies deception as an internal model property in order to improve detection and monitoring. However, the same techniques could in principle be misused: steering vectors that amplify deception could make models more persuasively dishonest, and understanding which layers encode deception could inform adversarial fine-tuning that evades probe-based detectors. I believe the defensive value outweighs these risks---detecting deception requires understanding it---but the dual-use nature of interpretability tools warrants caution in how steering vectors and deception-correlated features are shared and applied.

\section{Conclusion}

I presented a pipeline for studying deception representations in language models via social deduction game fine-tuning. The main contributions are: (1) probing with proper control baselines showing strong positive evidence for linearly accessible deception representations (97\% vs.\ 57\% control), and (2) providing supporting causal evidence via activation steering that the identified features causally influence model behavior---contrastive steering monotonically shifts probe predictions from 0\% to 100\% deception probability.

These results suggest that probe-based monitoring of deceptive intent in language model activations is a viable approach for AI safety, at least in domains where deception has a consistent behavioral signature.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
